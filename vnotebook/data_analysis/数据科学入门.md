# 数据科学入门

## 概述

内容较为丰富，且易懂，没有借助第三方库操作，加深对原理的理解，夯实基础。

主要内容包括：

1. 学到一堂 Python 速成课；
2. 学习线性代数、统计和概率论的基本方法，了解它们是怎样应用在数据科学中的；
3. 掌握如何收集、探索、清理、转换和操作数据；
4. 深入理解机器学习的基础；
5. 运用 k 近邻、朴素贝叶斯、线性回归和逻辑回归、决策树、神经网络和聚类等各种数据模型；
6. 探索推荐系统、自然语言处理、网络分析、MapReduce 和数据库。

## 可视化数据

### matplotlib

1. 条形图 bar

   ```python
   movies = ["Annie Hall", "Ben-Hur", "Casablanca", "Gandhi", "West Side Story"]
   num_oscars = [5, 11, 3, 8, 10]
   # 条形的默认宽度是 0.8，因此我们对左侧坐标加上 0.1
   # 这样每个条形就被放置在中心了
   xs = [i + 0.1 for i, _ in enumerate(movies)]
   # 使用左侧 x 坐标 [xs] 和高度 [num_oscars] 画条形图
   plt.bar(xs, num_oscars)
   plt.ylabel("所获奥斯卡金像奖数量")
   plt.title("我最喜爱的电影")
   # 使用电影的名字标记 x 轴，位置在 x 轴上条形的中心
   plt.xticks([i + 0.5 for i, _ in enumerate(movies)], movies)
   plt.show()
   ```

   > Tips: 使用合理的轴，使数据分布更合理。

2. 散点图 scatter

   ```python
   friends = [ 70, 65, 72, 63, 71, 64, 60, 64, 67]
   minutes = [175, 170, 205, 120, 220, 130, 105, 145, 190]
   labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i']
   plt.scatter(friends, minutes)
   # 每个点加标记
   for label, friend_count, minute_count in zip(labels, friends, minutes):
   	plt.annotate(label,
   		xy=(friend_count, minute_count), # 把标记放在对应的点上
   		xytext=(5, -5), # 但要有轻微偏离
   		textcoords='offset points')
   plt.title("日分钟数与朋友数")
   plt.xlabel("朋友数")
   plt.ylabel("花在网站上的日分钟数")
   plt.show()
   ```

   > Tips: 可以引入对 `plt.axis("equal")` 的调用，使数据分布在相同轴内，防止分布不合理引发的误导。

## 统计学

- `均值(mean)`：会随着数据变化而平稳变化，但是受异常值影响很大。
- `中位数(median)`：数据中间点的值，受异常值影响较小。
- `分位数(quantile)`：表示少于数据中特定百分比的一个值（中位数表示少于 50% 的数据的一个值）。
- `众数(mode)`：出现次数最多的一个或多个数。
- `极差(range)`：最大元素与最小元素的差，与中位数一样，极差也不真正依赖于整个数据集。
- `方差(variance)`：$var(X)=\frac{\sum_{i=1}^n{(X_i-\bar X)^2}}{n-1}$ ，衡量单个变量对均值的偏离程度。
- `标准差(standard deviation)`：$\sigma_{_X}=\sqrt{var(X)}$对方差进行开方，方差和标准差与均值一样对异常值敏感。
- `协方差(covariance)`：$cov(X,Y)=\frac{\sum_{i=1}^n(X_i-\bar X)(Y_i-\bar Y)}{n-1}$ ，衡量了两个变量对均值的串联偏离程度。
- `相关(correlation)`：$\rho_{_{X,Y}}=\frac{cov(X,Y)}{\sigma_{_X}\cdot\sigma_{_Y}}$ ，取值在 -1（完全反相关）和 1（完全相关）之间，相关系数的计算对异常值非常敏感。注意的是，相关系数是假设在 `其他条件都相同` 的前提之下衡量两个变量的关系。

## 概率

- `不相互独立`：事件 $E$ 发生意味着 $F$ 发生（或者 $F$ 发生意味着 $E$ 发生）。
- `相互独立`：与上述相反，事件 $E$ 的发生不影响事件 $F$ 的发生。
- `条件概率`：如果事件 $E$ 与事件 $F$ 独立，那么它们同时发生的概率为 $P(E,F)=P(E)P(F)$，则 $E$ 关于 $F$ 的条件概率为 $P(E|F)=P(E)$，可以理解为已知 $F$ 发生，$E$ 会发生的概率；如果两者不一定独立（并且 $F$ 的概率不为零），那么 $P(E|F)=P(E,F)/P(F)$ 。
- `贝叶斯定理`：事件 $F$ 可以分割为两个互不重合的时间“$F$ 和 $E$ 同时发生”与“$F$ 发生 $E$ 不发生”，则 $P(F)=P(F,E)+P(F,\neg E)$，因此 $P(E|F)=\frac{P(E,F)}{P(F)}=\frac{P(F|E)P(E)}{P(F)}=\frac{P(F|E)P(E)}{P(F|E)P(E)+P(F|\neg E)P(\neg E)}$。
- `随机变量`：指这样一种变量，其可能的取值有一种联合概率分布。联合分布对变量实现每种可能值都赋予了概率。
- `连续分布`：与离散分布（例如掷硬币）对应的概念，通常用带 `概率密度函数(probability density function, pdf)` 的连续分布来表示概率，一个变量位于某个区间的概率等于概率密度函数在这个区间的积分；另外 `累积分布函数(cumulative distribution function, cdf)` 给出了一个随机变量小于等于某一特定值的概率。
- `正态分布`：分布函数为 $f(x|\mu,\sigma)=\frac{1}{\sqrt{2\pi}\sigma}{\rm exp}\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$ ，当 $\mu=0,\sigma=1$ 时，这个分布为标准正态分布，如果 $Z$ 为标准正态分布，而 $X$ 为 $\mu,\sigma$ 的正态分布，则有 $Z=(X-\mu)/\sigma$  。
- `中心极限定理`：一个定义为大量独立同分布的随机变量的均值的随机变量本身就是接近于正态分布的，如果 $x_1,\cdots,x_n$ 都是均值为 $\mu$ 、标准差为 $\sigma$ 的随机变量，且 $n$ 很大，那么 $\frac1n(x_1+\cdots+x_n)$ 近似正态分布，且均值为 $\mu$，标准差为 $\sigma/\sqrt{n}$。等价于 $\frac{(x_1+\cdots+x_n)-\mu n}{\sigma\sqrt{n}}$ 近似标准正态分布。

## 假设与推断

典型的步骤：首先有一个 `零假设`$H_0$ ，它代表一个默认的立场，而替代假设 $H_1$ 代表我们希望与零假设对比的立场。然后通过统计来决定是否可以拒绝 $H_0$，即判断它是否错误。 

- `显著性`(significance)，其定义了我们有多大的可能性犯 `第 1 类错误`（容错）。在这种情况下，我们拒绝了原假设 $H_0$，但实际上原假设是正确的，该可能性的大小通常设定为 5% 或 1%。
- `势`(power)，检验的势指的是不犯 `第 2 类错误`（纳伪）的概率。第 2 类错误指原假设 $H_0$ 是错的，但是我们的检验结果没有拒绝原假设。
- `置信区间`(confidence interval)，比如检验某 $p$ 的可能性，所得到的区间表示在该区间内包含真实参数 $p$ 的可能性为 95%。这是关于区间的解释，而不是关于 $p$ 值的解释，即，如果你重复实验很多次，其中 95% 的“真”参数（每次都相同）会落在观测到的置信区间（每次可能会都不同）。

> 这部分内容需要加强理解，不是很明白。

## 获取数据

1. 利用 `sys.stdin` 和 `sys.stdout` 以 `管道(pipe)` 方式传递数据。

   例如，以下脚本按行读入文本，然后划分出和一个正则表达式匹配的行：

   ```python
   # egrep.py
   import sys, re
   
   # sys.argv 是命令行参数的列表
   # sys.argv[0] 是程序自己的名字
   # sys.argv[1] 会是在命令行上指定的正则表达式
   regex = sys.argv[1]
   
   # 对传递到这个脚本中的每一个行
   for line in sys.stdin:
   	# 如果它匹配正则表达式，则把它写入 stdout
   	if re.search(regex, line):
   		sys.stdout.write(line)
   ```

   然后对收到的行计数并输出计数结果：

   ```python
   # line_count.py
   import sys
   
   count = 0
   for line in sys.stdin:
   	count += 1
       
   # 输出去向 sys.stdout
   print(count)
   ```

   可以用这种方法来计数文件中有多少行包含数字。在 Windows 中，可以用：

   ```powershell
   type SomeFile.txt | python egrep.py "[0-9]" | python line_count.py
   ```

   在 Unix 系统中，可以用：

   ```shell
   cat SomeFile.txt | python egrep.py "[0-9]" | python line_count.py
   ```

   “ | ” 运算符是一个管道字符，它的意思是“使用左边命令的输出作为右边命令的输入”。

   类似的，下面的这个脚本计算了单词的数量并给出了最常用的单词：

   ```python
   # most_common_words.py
   import sys
   from collections import Counter
   
   # 传递单词的个数作为第一个参数
   try:
   	num_words = int(sys.argv[1])
   except:
   	print("usage: most_common_words.py num_words")
   	sys.exit(1) # 非零的 exit 代码表明有错误
   counter = Counter(word.lower()				# 小写的单词
   			for line in sys.stdin
   			for word in line.strip().split() # 用空格划分
   			if word)					# 跳过空的 'words'
   for word, count in counter.most_common(num_words):
       sys.stdout.write(str(count))
       sys.stdout.write("\t")
       sys.stdout.write(word)
       sys.stdout.write("\n")
   ```

2. 网络爬虫

## 数据工作

对于 `多维数据` 各个维度之间的相关性，一个简单的方法是考察 `相关矩阵(correlation matrix)`，矩阵中第 $i$ 行第 $j$ 列的元素表示第 $i$ 维与第 $j$ 维数据的相关性。

```python
def correlation_matrix(data):
    _, num_colums = shape(data)
    
    def matrix_entry(i, j):
        return correlation(get_column(data, i), get_column(data, j))
    
    return make_matrix(num_columns, num_columns, maxtrix_entry)
```

一个更为直观的方法（如果维度不太多）是做 `散点图矩阵`，以展示配对散点图。

```python
import matplotlib.pyplot as plt

_, num_columns = shape(data)
fig, ax = plt.subplots(num_columns, num_columns)

for i in range(num_columns):
    for j in range(num_columns):
        
        # x轴上column_j对y轴上column_i的散点
		if i != j:
            ax[i][j].scatter(get_column(data, j), get_column(data, i))
		
        # 只有当 i == j时显示序列名
		else:
            ax[i][j].annotate("series " + str(i), (0.5, 0.5),
							xycoords='axes fraction',
							ha="center", va="center")
		
        # 除了图的左侧和下方之外，隐藏图的标记
		if i < num_columns - 1:
            ax[i][j].xaxis.set_visible(False)
		if j > 0:
            ax[i][j].yaxis.set_visible(False)

# 修复右下方和左上方的图标记
# 因为它们只有文本，是错误的
ax[-1][-1].set_xlim(ax[0][-1].get_xlim())
ax[0][0].set_ylim(ax[0][1].get_ylim())

plt.show()
```

## 机器学习

如果划分训练集和测试集的目的不仅仅是为了判断模型，也是为了在许多模型中进行选择。此时，尽管不是所有的模型都是过拟合的，但“选择在测试集上表现最好的模型”是一种元训练 (meta-training)，会把测试集当作另一个训练集而运作。 (当然，在测试集上有最好表现的模型会在测试集上有持续的好表现。)

在这种情况下，应该把数据划分为三部分：一个用来建立模型的 `训练集`，一个为在训练好的模型上进行选择的 `验证集`，一个用来判断最终的模型的 `测试集`。

给定一个标签数据集和一个预测模型，可以得到 `混淆矩阵`(confusion matrix)：

|         |  实际 真   |  实际 假   |
| :-----: | :--------: | :--------: |
| 预测 真 | 真阳性(TP) | 假阳性(FP) |
| 预测 假 | 假阴性(FN) | 真阴性(TN) |

- `准确率(accuracy)`: 正确预测的比例，$accuracy=\frac{TP+TN}{TP+FP+FN+TN}$
- `查准率(precision)`: 度量模型关于“阳性”的预测的准确率，$precision=\frac{TP}{TP+FP}​$
- `查全率(recall)`: 度量模型所识别的“阳性”的比例，$recall=\frac{TP}{TP+FN}$
- `F1 score`: 将查准率和查全率组合起来，$F1=\frac{2\times precision\times recall}{precision+recall}$

F1 得分是查准率和查全率的调和平均值，因此必然会落在两者之间。

模型的选择通常是查准率和查全率之间的权衡。一个模型如果在信心不足的情况下预测“是”，那么它的查全率可能会比较高，但查准率较低；而如果一个模型在信心十足的情况下预测“是”，那么它的查全率可能会较低，但查准率却较高。

另一方面，也可以把这当作假阳性和假阴性之间的权衡。预测的“是”太多通常会给出很多的假阳性。预测的“否”太多通常会给出很多的假阴性。

## K 最近邻法

k 最近邻 (k-Nearest Neighbors, kNN) 算法是一种分类算法，基本思想为：一个样本与数据集中的 k 个样本最相似，如果这 k 个样本中的大多数属于某一个类别，则该样本也属于这个类别。

特点：

1. kNN 属于惰性学习，因为没有显式的学习过程，也就是说没有训练过程，数据集事先已有了分类和特征值，待收到新样本后直接进行处理；
2. kNN 的计算复杂度较高，新样本需要与数据集中每个数据进行距离计算，计算复杂度和数据集中的数据数目 n 成正比，时间复杂度为 O(n)，因此 kNN 一般适用于样本数较少的数据集；
3. k 取不同值时，分类结果可能会有显著不同，一般 k 的取值不超过 20，上限是 n 的开方。

## 朴素贝叶斯算法

贝叶斯分类是一类分类算法的总称，这类算法均以`贝叶斯定理`为基础，故统称为贝叶斯分类。朴素贝叶斯分类是一种十分简单的分类算法，思想基础为：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。

## 决策树

决策树 (decision tree) 是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，测试待分类项中响应的特征属性， 并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。

大多数人都将决策树分为 `分类决策树`(classification tree，它输出的是判决结果) 和 `回归决策树`(regression tree，它输出的是数值结果)。

用 `熵(entropy)` 指代“信息含量”，用来表示与数据相关的不确定性。假设有一个数据集 $S$，每个数据元素都标明了所属的类别，即元素属于有限类别 $C_1,\cdots,C_n$ 中的一种。如果所有数据点都属于同一类别，那么也就不存在不确定性了，这就属于低熵情形。如果数据点均匀地分布在各个类别中，那么不确定性就较大，这时就称具有较大的熵。

从数学的角度来讲，如果 $p_i$ 表示 $c_i$ 类别中的数据所占的比例，那么可以把熵定义为：
$$
H(S)=-p_1\log_2p_1-\cdots-p_n\log_2p_n
$$
我们希望通过某种方法对数据集的分割效果来表示熵。对于某个划分方法，如果得到的子集的熵较低（即确定性很高）的话，我们就说这个划分方法的熵较低；反之，如果得到的子集的（数量很多并且）熵较高（即不确定性很高）的话，我们就说这个划分方法得到的熵较高。

在数学上，如果我们把数据集 $S$ 划分为数据子集 $S_1, \cdots,S_m$，各个子集相应数据量所占比例为 $q_1,\cdots,q_m$，那么我们就可以通过如下加权和的形式来计算这次划分的熵：
$$
H = q_1H(S_1)+\cdots+q_mH(S_m)
$$
由于决策树与其训练数据的契合程度非常高，因此，它总是倾向于出现过拟合现象。为了避免出现这种情况，可以使用 `随机森林(random forest)` 技术。利用该技术，可以建立多个决策树，然后通过投票方式决定如何对输入进行分类。

## k-均值 (k-means) 算法

k-均值算法是一种最简单的聚类分析方法，它通常需要首先选出聚类 k 的数目，然后把输入划分为集合 $S_1,\cdots,S_k$，并使得每个数据到其所在聚类的均值（中心对象）的距离的平方之和最小化。其一般步骤为：

1. 首先从 d 维空间中选出 k 个数据点作为初始聚类的均值（即中心）。

2. 计算每个数据点到这些聚类的均值（即聚类中心）的距离，然后把各个数据点分配给离它最近的那个聚类。

3. 如果所有数据点都不再被重新分配，那么就停止并保持现有聚类。

4. 通过仍有数据点被重新分配，则重新计算均值，并返回第 2 步。

## 自下而上的分层聚类

采用自下而上的方式“培养”聚类，为此，可以借助下列方式：

1. 利用每个输入构成一个聚类，当然每个聚类只包含一个元素；
2. 只要还剩余多个聚类，就找出最接近的两个，并将它们合二为一。

最后，我们将得到一个包含所有输入的巨大的聚类。如果我们将合并顺序记录下来，就可以通过拆分的方法来重建任意数量的聚类。

## MapReduce

MapReduce 的主要优点就是通过处理过程移动到数据来进行分布式的计算。